{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project \n",
    "================================================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TEAM**: Star Li, Jacky Zhang, Stefan Li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style = \"whitegrid\", \n",
    "        color_codes = True,\n",
    "        font_scale = 1.5)\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.linear_model import LassoCV, LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor, ExtraTreesRegressor, RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_state = pd.read_csv('final_proj_data/covid19/4.18states.csv')\n",
    "data_counties = pd.read_csv('final_proj_data/covid19/abridged_couties.csv')\n",
    "data_time_conf = pd.read_csv('final_proj_data/covid19/time_series_covid19_confirmed_US.csv')\n",
    "data_time_de = pd.read_csv('final_proj_data/covid19/time_series_covid19_deaths_US.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Based on our fist look at the data as well as the objective of our project, \n",
    "# we decided to work with only the U.S. data since we have the most specific data \n",
    "# regarding all the cases in the U.S. \n",
    "data_state = data_state.loc[data_state['Country_Region'] == 'US']\n",
    "data_state = data_state.rename(columns={\"Long_\": \"Long\"})\n",
    "\n",
    "data_state.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. \"4.18state\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fist divide the data into edge case (e.g. Diamond Princess that does not have lat and long) and general cases\n",
    "\n",
    "edge_case = data_state[data_state['Lat'].isnull()]\n",
    "edge_case.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that there are only three cases in the edge_case category, and those are very specially cases \n",
    "# such as \"Diamond Princess\" and \"Grand Princess\", and the are clearly not a good representation of regional cases.\n",
    "#So, we can safely drop these columns from our main focuse\n",
    "\n",
    "general_state = data_state[data_state['Lat'].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning up NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_state[general_state['Recovered'].isnull()].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Notice that there are some NaN's columns such as \"Recovered\", \"Active\", \"Mortality_rate\", and \"Hospitalization_Rate\"\n",
    "- We know that, based on the provided README file, Active cases = total confirmed - total recovered - total deaths, we can safely fill out all the NaN's in Deaths, Recorvered, and Active with 0, since they are all mutually exclusive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_state[['Recovered']] = general_state[['Recovered']].fillna(0)\n",
    "general_state[['Active']] = general_state[['Active']].fillna(0)\n",
    "general_state[['Mortality_Rate']] = general_state[['Mortality_Rate']].fillna(0)\n",
    "\n",
    "# Notice that US Hospitalization Rate (%): = Total number hospitalized / Number confirmed cases,\n",
    "# so if the number of People_Hospitalized is NaN or 0, \n",
    "# we can logically fill in 0 for all the NaN's in these two columns\n",
    "\n",
    "general_state[['People_Hospitalized']] = general_state[['People_Hospitalized']].fillna(0)\n",
    "general_state[['Hospitalization_Rate']] = general_state[['Hospitalization_Rate']].fillna(0)\n",
    "\n",
    "general_state.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Up to this point, we see that there is only one NaN value left in the column of Last_Update\n",
    "\n",
    "general_state[general_state['Last_Update'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since \"American Samoa\" has such a small number of cases, it will have relatively small effect on our prediction, \n",
    "# so we decided to remove this area to keep the consistency of our dataframe.\n",
    "\n",
    "general_state = general_state[general_state['Last_Update'].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Joining \"abridged_couties\" with \"time_series_covid19_confirmed_US\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- abridged_counties contains wonderful information about population health condition as well as population demograpics, and time_series_covid19_confirmed_US has a wonderful pattern of the times series of confirms in the U.S., so it would be a lot easier to select feature from this join table rather than subseting data separately from each data set when selecting features.\n",
    "- Notice that we are not cleaning up the data for the death timeserise, as our main objective is to predict the confirmed cases of the next day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- We first filtered out the territories of the U.S. as we think it \n",
    "# is not the best representation of the cases of all the other 50 major U.S. States\n",
    "\n",
    "data_time_conf = data_time_conf.iloc[5:, :]\n",
    "data_counties = data_counties.iloc[:-2, :]\n",
    "\n",
    "data_counties.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are two rows with all NaN's at the end, we the code above dropped them.\n",
    "- We decided to use \"UID\" of data_time_conf and \"countyFISP\" from data_counties as our foreign key, and notice that the \"countyFIPS\" of data_counties is just UID of data_time_conf add 84000000\n",
    "- So in the following codes, we adjust the keys such that they will match with each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_time_conf['UID'] = data_time_conf['UID'].astype(int)\n",
    "data_counties[\"countyFIPS\"] = data_counties['countyFIPS'].astype(int)\n",
    "data_counties['countyFIPS'] = data_counties['countyFIPS'] + 84000000\n",
    "\n",
    "# Merge the two table\n",
    "combined_conf = data_time_conf.merge(data_counties, left_on= 'UID', right_on = 'countyFIPS')\n",
    "\n",
    "combined_conf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning up NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_conf.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Notice that there are lots of NaN values in some of the columns. \n",
    "- Here, in order to give a consistant and relatively accurate X matrix for fitting our models, we decided to drop out the columns with too many NaNs, since if there are too many NaNs in a category, it would be really hard to choose alternative for them since demographic information are very unique to each state, and filling up all the NaN data with \"mean\" \"median\" or \"most frequent\" will potentially bring misleading information to our models.\n",
    "- To define \"too many\", we decided that if more than 20% of the data are NaN, we will drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_conf.loc[:, combined_conf.isnull().mean() > .2].isnull().mean()\n",
    "combined_conf = combined_conf.loc[:, combined_conf.isnull().mean() <= .2]\n",
    "combined_conf.loc[:, combined_conf.isnull().mean() > 0].isnull().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns to Drop Direactly:\n",
    "1. `State`: The Province_State from the time serise table has no NaN values, so Provinc_State can already represent the names of all the State.\n",
    "2. `lat`: The Lat from the time serise table has no NaN values, so it can present all the locations\n",
    "3. `lon`: The Lon from the time serise table has no NaN values, so it can present all the locations\n",
    "4. `entertainment/gym`: We think gym and entertainment might have long-term effect for human body, but it is not very related to the virus that is happend right now.\n",
    "\n",
    "Columns to take the \"Mean\" value to fill NaNs:\n",
    "1. `#EligibleforMedicare2018`: Since it has such a small NaN percentage and Medicare system is relatively well-developed in the U.S., we decided to fill the mean for the NaN of this column\n",
    "2. `All the rate, ratio, and percentile`: Since rate and ratio has already scaled, we can safely apply rate and ratio to states and counties with different populations.\n",
    "3. `All the MortalityAge`: Since all of them have such low NaN precentage, we decided to fill in Mean for them, as Mean will average out the effect of states with large population and small population. \n",
    "\n",
    "Columns to take the \"0\" value to fill NaNs:\n",
    "1. `>50 gatherings`: since gatherings might have a huge impact on confirmed cases, we rather to do it more safely by filling in 0s for these features with NaNs.\n",
    "2. `>500 gatherings`: since gatherings might have a huge impact on confirmed cases, we rather to do it more safely by filling in 0s for these features with NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_conf = combined_conf.drop(columns = ['State', 'lat', 'lon', 'entertainment/gym'])\n",
    "combined_conf['>50 gatherings'] = combined_conf['>50 gatherings'].fillna(0)\n",
    "combined_conf['>500 gatherings'] = combined_conf['>500 gatherings'].fillna(0)\n",
    "combined_conf = combined_conf.fillna(combined_conf.mean())\n",
    "# Indicator that there are no more NaN left in our dataframe!\n",
    "combined_conf.loc[:, combined_conf.isnull().mean() > 0].isnull().mean().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create the All-in-1 DataFrame for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_in_1 = combined_conf.merge(general_state, left_on = 'Province_State', right_on='Province_State')\n",
    "\n",
    "all_in_1.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Notice there are many columns within these two dataframe with repeated names, which Pandas matigate through it with adding \"_x\" \"_y\" at the end of the column names\n",
    "- So, I clean out the data further by dropping the columns with overlapping names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['UID_x', 'iso2', 'iso3', \"code3\", \"FIPS_x\", 'Admin2', \n",
    "                'Province_State', \"Country_Region_x\", \"Combined_Key\",\n",
    "               \"StateName\", 'countyFIPS', 'STATEFP', 'COUNTYFP', 'CountyName', \n",
    "                'POP_LATITUDE', 'POP_LONGITUDE', 'FIPS_y', \"UID_y\", \n",
    "                \"ISO3\", 'CensusRegionName', 'Country_Region_y', 'Last_Update', 'CensusDivisionName']\n",
    "\n",
    "all_in_1 = all_in_1.drop(columns = drop_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA & Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_test, y_true):\n",
    "    return np.mean((y_test - y_true) ** 2)\n",
    "\n",
    "def expo_fit(y):\n",
    "    x = np.arange(-y.shape[0], 0)\n",
    "    return np.exp(np.polyfit(x, np.log(y), 1)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import the data that has already been cleaned and merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"all_in_1.csv\")\n",
    "X, y = data.drop(columns=['4/18/20']), data['4/18/20']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Data Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exponential Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for j in range(86,89):\n",
    "    result = []\n",
    "    for i in range(j-12, j-2):\n",
    "        y_mat = X.iloc[:,i:j].to_numpy()\n",
    "        exp_predict = np.apply_along_axis(expo_fit, 1, y_mat)\n",
    "        exp_predict[np.isnan(exp_predict)] = 0\n",
    "        result.append(mse(exp_predict, X.iloc[:,j]))\n",
    "    results.append(result)\n",
    "\n",
    "# special handling for interpolating 4/18/20 data\n",
    "result = []\n",
    "for i in range(78, 88):\n",
    "    y_mat = X.iloc[:,i:90].to_numpy()\n",
    "    exp_predict = np.apply_along_axis(expo_fit, 1, y_mat)\n",
    "    exp_predict[np.isnan(exp_predict)] = 0\n",
    "    result.append(mse(exp_predict, y))\n",
    "\n",
    "results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(11, 1, -1)\n",
    "labels = ['4/15/20', '4/16/20', '4/17/20', '4/18/20']\n",
    "plt.figure(figsize=(8, 4))\n",
    "for i in range(4):\n",
    "    plt.plot(x, results[i], label=labels[i])\n",
    "plt.legend()\n",
    "plt.ylabel(\"mse of interpolation\")\n",
    "plt.xlabel(\"# of days used in interpolation\")\n",
    "plt.title(\"Figure C: performance of exponential interpolation v.s. # of previous days used\", fontsize=11);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first assumption follows from the **common exponential model for epidemic growth**. We take advantage of this and try to answer \"which days in the time series can accurately predict the future\" using exponential interpolation. More specifically, to generalize this assumption and reduce our search space, we **fit exponential curves for \"n consecutive days earlier\" ($n \\in [2, 11]$).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution Observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.distplot(y[y < np.percentile(y, 80)], kde=False)\n",
    "plt.xlabel(\"Confirmed cases in 4/18/20\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Figure A: distribution of confirmed cases for counties < 80 percentile\", fontsize=15);\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.scatterplot(x=\"4/18/20\", y=\"PopulationEstimate2018\", data=data)\n",
    "plt.xlabel(\"Confirmed cases in 4/18/20\")\n",
    "plt.title(\"Figure B: distplot of confirmed cases v.s. population\", fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In figure A, we plot out the **distribution of confirmed cases for counties within lower 80 percentiles**. It shows how left-skewed the distribution is as **80\\% of counties have less than 80 cases and more than $\\frac{1}{3}$ of counties have less than 10**. \n",
    "- Figure B gives us a more complete view of the distribution of populations and confirmed cases, as we can see **the clusters of points near the origin and some \"outlier-like\" points far away**, which helps us greatly in the downstream model selection. We also didn't find a strong correlation between population size and the confirmed case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. LassoCV Automated Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfm = SelectFromModel(LassoCV(normalize=True)).fit(X, y)\n",
    "\n",
    "feature_imp_df = pd.DataFrame({\"col_name\": X.columns, \"imp\": sfm.estimator_.coef_})\n",
    "feature_imp_df.sort_values(by=\"imp\", ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Correlation Visualization Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_features = ['dem_to_rep_ratio', 'PopulationDensityperSqMile2010', 'PopFmle25-292010', '4/18/20']\n",
    "extra_t_features = ['PopulationEstimate2018', 'stay at home', \n",
    "                    'Mortality_Rate', 'Testing_Rate', 'People_Hospitalized', \n",
    "                    'Incident_Rate', 'People_Tested', 'StrokeMortality','PopFmle65-742010',\n",
    "                    'PopMale75-842010', 'PopFmle75-842010', \n",
    "                    'PopFmle45-542010','PopMale55-592010', 'PopFmle55-592010', 'PopMale60-642010',\n",
    "                    'PopFmle60-642010',\n",
    "                    'PopMale75-842010', 'PopFmle75-842010',\n",
    "                    '3-YrMortalityAge55-64Years2015-17',\n",
    "                    '3-YrMortalityAge65-74Years2015-17',\n",
    "                    '3-YrMortalityAge75-84Years2015-17', '3-YrMortalityAge85+Years2015-17',]\n",
    "heat_features = lasso_features + extra_t_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "heat_df = all_in_1[heat_features]\n",
    "\n",
    "plt.figure(figsize=(10, 9))\n",
    "plot = sns.heatmap(heat_df.corr()[['4/18/20']].sort_values(by=['4/18/20'],ascending=False),\n",
    "            cmap=\"YlGnBu\", linewidths=.5, annot=True, annot_kws={\"size\": 10})\n",
    "plot.set_title('Top Correlations between Confirmed cases of 4/18/20 with other Features');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['4/15/20',\n",
    " '4/16/20',\n",
    " '4/17/20',\n",
    " 'dem_to_rep_ratio',\n",
    " 'public schools',\n",
    " 'FracMale2017',\n",
    " 'DiabetesPercentage',\n",
    " 'People_Tested',\n",
    " 'HeartDiseaseMortality',\n",
    " ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Simple Regressors (LinearRegression v.s. DecisionTree vs. KNN Regressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Augmentation Attempt using KNN Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_metric(loc1, loc2):\n",
    "    lat1, lon1, lat2, lon2  = loc1[0], loc1[1], loc2[0], loc2[1]\n",
    "    p = 0.017453292519943295 # Pi / 180\n",
    "    a = 0.5 - np.cos((lat2 - lat1) * p) / 2 + np.cos(lat1 * p) * np.cos(lat2 * p) * \\\n",
    "    (1 - np.cos((lon2 - lon1) * p)) / 2\n",
    "    return 12742 * np.arcsin(a ** 2) # 12742 = 2 * R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_KNN, y_KNN = data.loc[:, ['Lat_x', 'Long_']], data['4/18/20']\n",
    "\n",
    "neigh = KNeighborsRegressor(n_neighbors=2, weights = 'distance', metric = dist_metric)\n",
    "neigh.fit(X_KNN, y_KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up a dictionary with {county_index: list of its K Nearest Neighbours} Pairings\n",
    "clusters = neigh.kneighbors()[1]\n",
    "neighbour_dict = {i: clusters[i] for i in X.index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref_pop is the poplation of the county being referenced to, neigh_pop is the population of one of its \n",
    "# neighbor counties counts, neigh_pops are np arrays, ref_pop is is an integer\n",
    "def normalize_by_pop(stat_mat, neigh_pops, ref_pop):\n",
    "    factors = [ref_pop/n_pop for n_pop in neigh_pops]\n",
    "    weighted_sums = stat_mat @ factors\n",
    "    return list(weighted_sums / len(neigh_pops))\n",
    " \n",
    "\n",
    "f = ['4/15/20','4/16/20','4/17/20']\n",
    "f_new = ['4/15/20_neigh_nor','4/16/20_neigh_nor','4/17/20_neigh_nor']\n",
    "\n",
    "new_feature_list = []\n",
    "\n",
    "for i in X.index:\n",
    "    neigh_id = neighbour_dict.get(i)\n",
    "\n",
    "    ref_pop = X.loc[i, 'PopulationEstimate2018']\n",
    "    neigh_pops = X.loc[neigh_id, 'PopulationEstimate2018']\n",
    "    stat_matrix = np.transpose(X.loc[neigh_id, f]) # a len(f) by num_neigh matrix\n",
    "    \n",
    "    new_feature_list.append(normalize_by_pop(stat_matrix, neigh_pops, ref_pop))\n",
    "\n",
    "new_feature_df = pd.DataFrame(np.array(new_feature_list), columns=f_new)\n",
    "\n",
    "X_new = pd.concat([X, new_feature_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X[features], y, test_size=0.1, random_state=42)\n",
    "\n",
    "X_train_aug, X_test_aug, y_train_aug, y_test_aug = train_test_split(X_new[features + f_new], y, test_size=0.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall MSE of LinearRegressor and TreeRegresso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regr = LinearRegression(normalize=True)\n",
    "linear_regr.fit(X_train, y_train)\n",
    "\n",
    "tree_regr = DecisionTreeRegressor(max_depth=20, random_state=42)\n",
    "tree_regr.fit(X_train, y_train)\n",
    "\n",
    "linear_regr_KNN = LinearRegression(normalize=True)\n",
    "linear_regr_KNN.fit(X_train_aug, y_train_aug)\n",
    "\n",
    "tree_regr_KNN = DecisionTreeRegressor(max_depth=20, random_state=42)\n",
    "tree_regr_KNN.fit(X_train_aug, y_train_aug)\n",
    "\n",
    "\n",
    "linear_pred = np.round(np.maximum(0, linear_regr.predict(X_test)))\n",
    "linear_mse = mse(linear_pred, y_test)\n",
    "\n",
    "tree_pred = np.round(tree_regr.predict(X_test))\n",
    "tree_mse = mse(tree_pred, y_test)\n",
    "\n",
    "KNN_linear_pred = np.round(np.maximum(0, linear_regr_KNN.predict(X_test_aug)))\n",
    "KNN_linear_mse = mse(KNN_linear_pred, y_test_aug)\n",
    "\n",
    "KNN_tree_pred = np.round(tree_regr_KNN.predict(X_test_aug))\n",
    "KNN_tree_mse = mse(KNN_tree_pred, y_test_aug)\n",
    "\n",
    "\n",
    "print(f\"Linear mse: {linear_mse}, Tree mse: {tree_mse}\")\n",
    "print(f\"Linear_KNN mse: {KNN_linear_mse}, Tree_KNN mse: {KNN_tree_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that that the KNN Algorithm act as a mediator for the algorithms. It improves the Desion Tree Model quite significantly, by introducing noise, nuetralizing overfitting.\n",
    "- Our data, concluded from EDA, can be better modeled by a linear model. Thus, KNN adds noise to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the mse for prediction of counties with small/middle/large confirmed cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_25, per_75 = np.percentile(y_test, 25), np.percentile(y_test, 75)\n",
    "\n",
    "small_indices = np.argwhere(y_test < per_25).flatten()\n",
    "middle_indices = np.argwhere((y_test >= per_25) & (y_test < per_75)).flatten()\n",
    "big_indices = np.argwhere(y_test >= per_75).flatten()\n",
    "\n",
    "for indices in [small_indices, middle_indices, big_indices]:\n",
    "    a = y_test.to_numpy()[indices]\n",
    "    \n",
    "    #Linear\n",
    "    b = linear_pred[indices]\n",
    "    # Decision Tree\n",
    "    c = tree_pred[indices]\n",
    "    #Linear + KNN\n",
    "    d = KNN_linear_pred[indices]\n",
    "    # Decision Tree + KNN\n",
    "    e = KNN_tree_pred[indices]\n",
    "    \n",
    "    print(f\"Linear mse: {mse(a, b)}, Tree mse: {mse(a, c)}\") \n",
    "    print(f\"Linear_KNN mse: {mse(a, d)}, Tree_KNN mse: {mse(a, e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see the above tendency is more obvious data is bigger. This give a nice intuition for fine tuning the final model: applying KNN to the larger percentile of data might be benefitial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Final Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class finalRegressor(object):\n",
    "    def __init__(self, threshold=50, depth=11, forest=False, randstate=42):\n",
    "        self.thres = threshold\n",
    "        if forest:\n",
    "            self.tree_regr = RandomForestRegressor(n_estimators=50, max_depth=depth, random_state=randstate)\n",
    "        else:\n",
    "            self.tree_regr = DecisionTreeRegressor(max_depth=depth, random_state=randstate)\n",
    "        self.linear_regr = LinearRegression(normalize=True)\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        self.tree_regr.fit(X_train, y_train)\n",
    "        self.linear_regr.fit(X_train, y_train)\n",
    "        self.thres_val = np.percentile(y_train, self.thres)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        self.linear_pred = np.maximum(self.linear_regr.predict(X_test), 0)\n",
    "        self.tree_pred = self.tree_regr.predict(X_test)\n",
    "        self.cond_vec = ((self.linear_pred + self.tree_pred) / 2) < self.thres_val\n",
    "        return np.round(self.tree_pred * self.cond_vec + self.linear_pred * ~self.cond_vec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using test data to tune the max_depth for DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_results = []\n",
    "small_indices = np.argwhere(y_test < 100).flatten()\n",
    "\n",
    "for depth in range(3, 30):\n",
    "    dt = DecisionTreeRegressor(max_depth=depth)\n",
    "    dt.fit(X_train, y_train)\n",
    "    a, b = dt.predict(X_test)[small_indices], y_test.to_numpy()[small_indices]\n",
    "    depth_results.append(mse(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_max_depth = np.argmin(depth_results) + 3\n",
    "optimal_max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(depth_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using 5-fold Cross Validation to tune the best threshold of our final predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "best_thres, best_val = None, float('inf')\n",
    "\n",
    "for thres in range(20, 100):\n",
    "    cv_result = []\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        final_regr = finalRegressor(thres)\n",
    "        final_regr.fit(X_train.iloc[train_index], y_train.iloc[train_index])\n",
    "        final_pred = final_regr.predict(X_train.iloc[test_index])\n",
    "        cv_result.append(mse(final_pred, y_train.iloc[test_index]))\n",
    "    if best_val > np.mean(cv_result):\n",
    "        best_val = np.mean(cv_result)\n",
    "        best_thres = thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictor = finalRegressor(best_thres) \n",
    "final_predictor.fit(X_train, y_train)\n",
    "best_mse = mse(y_test, final_predictor.predict(X_test))\n",
    "print(\"Our integrated model, after fine-tuning, achieves the MSE of \" + str(best_mse) + \" on testing set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "================================================================================================================\n",
    "# END PROJECT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
